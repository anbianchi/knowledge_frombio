{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "import random\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "#visualization\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "from IPython.display import Image\n",
    "from IPython.display import HTML\n",
    "\n",
    "#scispacy\n",
    "import scispacy\n",
    "import spacy\n",
    "import en_core_sci_scibert   \n",
    "#import en_ner_bionlp13cg_md\n",
    "from spacy import displacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "#relation extraction\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from zero_shot_re import RelTaggerModel, RelationExtractor\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a dictionary to store all the knowledge graphs\n",
    "knowledge_graphs = {}\n",
    "\n",
    "# Define a function to construct an interactive knowledge graph\n",
    "def construct_knowledge_graph(report, relations):\n",
    "    # Choose a random color for the knowledge graph\n",
    "    colors = ['blue', 'green', 'yellow', 'orange', 'purple', 'pink']\n",
    "    color = random.choice([c for c in colors if c not in knowledge_graphs.values()])\n",
    "    \n",
    "    # Create a new knowledge graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for entity in report[0][\"entities\"]:\n",
    "        G.add_node(entity[\"entity_id\"], label=entity[\"entity\"], color=color)\n",
    "    \n",
    "    # Add edges\n",
    "    for relation in relations:\n",
    "        print(relation)\n",
    "        print(relation.keys())\n",
    "        G.add_edge(relation[\"entity1_id\"], relation[\"entity2_id\"], label=relation[\"relation_type\"], color=color)\n",
    "        \n",
    "    return G\n",
    "    \n",
    "# Define a function to merge and solve conflicts in knowledge graphs\n",
    "def merging_and_solving_conflicts():\n",
    "    # Merge all the knowledge graphs\n",
    "    merged_graph = nx.DiGraph()\n",
    "    for graph in knowledge_graphs.values():\n",
    "        merged_graph = nx.compose(merged_graph, graph)\n",
    "    \n",
    "    # Find all the common entities\n",
    "    entity_counts = defaultdict(int)\n",
    "    for node in merged_graph.nodes:\n",
    "        entity_counts[node] += 1\n",
    "    common_entities = [entity for entity, count in entity_counts.items() if count > 1]\n",
    "    \n",
    "    # Resolve conflicts in common entities\n",
    "    for entity in common_entities:\n",
    "        # Find all the nodes with the same entity\n",
    "        nodes = [node for node in merged_graph.nodes if node == entity]\n",
    "        \n",
    "        # Find the color of the first node\n",
    "        color = None\n",
    "        for node in nodes:\n",
    "            if node in knowledge_graphs:\n",
    "                color = knowledge_graphs[node]\n",
    "                break\n",
    "        \n",
    "        # Change the color of all the nodes to the first color\n",
    "        for node in nodes:\n",
    "            if node in knowledge_graphs:\n",
    "                knowledge_graphs[node] = color\n",
    "            merged_graph.nodes[node]['color'] = color\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scispacy_model():\n",
    "    nlp = spacy.load(\"en_core_sci_scibert\")\n",
    "    nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n",
    "    return nlp\n",
    "\n",
    "def load_bern2_model(preprocessed_reports):\n",
    "    entity_list = []\n",
    "    try:\n",
    "        entity_list.append(requests.post(\"http://bern2.korea.ac.kr/plain\", json={'text': preprocessed_reports}).json())\n",
    "        #entity_list[0]['annotations'].extend(entity_list['annotations'])\n",
    "    except:\n",
    "        print('invalid sentence')\n",
    "        \n",
    "    return entity_list\n",
    "\n",
    "def load_relation_extraction_model():\n",
    "    model = RelTaggerModel.from_pretrained(\"fractalego/fewrel-zero-shot\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "    relations = ['associated', 'interacts']\n",
    "    extractor = RelationExtractor(model, tokenizer, relations)\n",
    "    return extractor\n",
    "\n",
    "def extract_entities_scispacy(preprocessed_reports):\n",
    "    nlp = load_scispacy_model()\n",
    "    doc = nlp(preprocessed_reports)\n",
    "           \n",
    "    \"\"\" entity = doc.ents[1]\n",
    "\n",
    "    print(\"Name: \", entity)\n",
    "    >>> Name: bulbar muscular atrophy\n",
    "\n",
    "    # Each entity is linked to UMLS with a score\n",
    "    # (currently just char-3gram matching).\n",
    "    linker = nlp.get_pipe(\"scispacy_linker\")\n",
    "    for umls_ent in entity._.kb_ents:\n",
    "        print(linker.kb.cui_to_entity[umls_ent[0]])          \n",
    "               \"\"\"\n",
    "          \n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entity = {\n",
    "            'entity_id': ent.text,\n",
    "            'other_ids': [],\n",
    "            'entity_type': ent.label_,\n",
    "            'entity': ent.text\n",
    "        }\n",
    "        entities.append(entity)\n",
    "\n",
    "    candidates = {\n",
    "    'entities': entities,\n",
    "    'text': doc.text,\n",
    "    'text_sha256': hashlib.sha256(doc.text.encode()).hexdigest()\n",
    "    }\n",
    "    return candidates\n",
    "\n",
    "def extract_entities_bern2(preprocessed_reports):\n",
    "    entity_list = load_bern2_model(preprocessed_reports)\n",
    "    parsed_entities = []\n",
    "    for entities in entity_list:\n",
    "        e = []\n",
    "        if not entities.get('annotations'):\n",
    "            parsed_entities.append({'text':entities['text'], 'text_sha256': hashlib.sha256(entities['text'].encode('utf-8')).hexdigest()})\n",
    "            continue\n",
    "        for entity in entities['annotations']:\n",
    "            other_ids = [id for id in entity['id'] if not id.startswith(\"BERN\")]\n",
    "            entity_type = entity['obj']   \n",
    "            entity_prob = entity['prob']                                                          \n",
    "            entity_name = entities['text'][entity['span']['begin']:entity['span']['end']]\n",
    "            try:\n",
    "                entity_id = [id for id in entity['id'] if id.startswith(\"BERN\")][0]\n",
    "            except IndexError:\n",
    "                entity_id = entity_name\n",
    "            e.append({'entity_id': entity_id, 'other_ids': other_ids, 'entity_type': entity_type, 'entity': entity_name, 'entity_prob': entity_prob})\n",
    "\n",
    "    parsed_entities.append({'entities':e, 'text':entities['text'], 'text_sha256': hashlib.sha256(entities['text'].encode('utf-8')).hexdigest()})\n",
    "    \n",
    "    #return entity_list\n",
    "    return parsed_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Bio_ClinicalBERT_model():\n",
    "    model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Define a function to extract relations between entities in a report\n",
    "def extract_relations(report, model, tokenizer):\n",
    "    # Tokenize the report\n",
    "    inputs = tokenizer(report[\"text\"], return_tensors=\"pt\")\n",
    "    # Generate embeddings for each entity in the report\n",
    "    entity_embeddings = []\n",
    "    for entity in report[\"entities\"]:\n",
    "        entity_text = entity[\"entity\"]\n",
    "        entity_inputs = tokenizer(entity_text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            entity_output = model(**entity_inputs)[0][:, 0, :]\n",
    "        entity_embeddings.append(entity_output)\n",
    "    # Compute the similarity between each pair of entity embeddings\n",
    "    relations = []\n",
    "    for i, e1 in enumerate(entity_embeddings):\n",
    "        for j, e2 in enumerate(entity_embeddings):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                similarity = torch.cosine_similarity(e1, e2).item()\n",
    "            # If the similarity is above a threshold, predict a relation\n",
    "            if similarity > 0.85:\n",
    "                relation = {\n",
    "                    \"entity1_id\": report[\"entities\"][i][\"entity_id\"],\n",
    "                    \"entity2_id\": report[\"entities\"][j][\"entity_id\"],\n",
    "                    \"relation_type\": \"associated\",\n",
    "                    \"confidence\": similarity,\n",
    "                }\n",
    "                relations.append(relation)\n",
    "    return relations\n",
    "\n",
    "def visualize_knowledge_graph(G):\n",
    "    # Visualize the knowledge graph\n",
    "    nt = Network(height='800px', width='100%', font_color='black')\n",
    "    for node, data in G.nodes(data=True):\n",
    "        nt.add_node(node, label=data['label'], color=data['color'])\n",
    "    for source, target, data in G.edges(data=True):\n",
    "        nt.add_edge(source, target, title=data['label'])\n",
    "    html = nt.show('notebook.html')\n",
    "    display(HTML(html)) \n",
    "    \n",
    "    # TERMINAL COMPATIBILITY\n",
    "    #file_path = \"knowledge_graph.html\"\n",
    "    #nt.save_graph(file_path)\n",
    " \n",
    "\"\"\" def visualize_knowledge_graph(G):\n",
    "    pos = nx.spring_layout(G)\n",
    "    labels = {node: G.nodes[node]['label'] for node in G.nodes()}\n",
    "    node_colors = [G.nodes[node]['color'] for node in G.nodes()]\n",
    "    edge_labels = {(u, v): G[u][v]['label'] for u, v in G.edges()}\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    nx.draw(G, pos, labels=labels, with_labels=True, node_size=3000, node_color=node_colors)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "    plt.show() \"\"\"   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_medical_knowledge():\n",
    "    # Load the medical knowledge as a list of strings\n",
    "    medical_knowledge = []\n",
    "    preprocessed_reports = []\n",
    "    for filename in os.listdir(\"diagnostic_reports\"):\n",
    "        with open(os.path.join(\"diagnostic_reports\", filename), \"r\") as f:\n",
    "            text = f.read()\n",
    "            # Pre-process the medical knowledge by lowercasing and removing special characters\n",
    "            text = re.sub(r'[^a-zA-Z0-9.\\s]', '', text).lower()\n",
    "            medical_knowledge.append(text)\n",
    "            \n",
    "            preprocessed_reports.append(text)\n",
    "    \n",
    "    return preprocessed_reports\n",
    "\n",
    "def process_reports(preprocessed_reports):\n",
    "    re_model, re_tokenizer = load_Bio_ClinicalBERT_model()\n",
    "\n",
    "    preprocessed_reports = preprocess_medical_knowledge()\n",
    "    # Extracting entities using biobern2\n",
    "    entity_lists_bern2 = []\n",
    "\n",
    "    for report in preprocessed_reports:\n",
    "        entity_lists_bern2.append(extract_entities_bern2(report))\n",
    "    #print(entity_lists_bern2)   \n",
    "\n",
    "    # Extract relations between entities in each report in entity_lists_bern2\n",
    "    predicted_rels = []\n",
    "    for entity_list in entity_lists_bern2:\n",
    "        for report in entity_list:\n",
    "            relations = extract_relations(report, re_model, re_tokenizer)\n",
    "            predicted_rels.append(relations)\n",
    "            \n",
    "    #print(predicted_rels)\n",
    "\n",
    "    # Generate and merge knowledge graphs for all reports\n",
    "    G_all = nx.Graph()\n",
    "    for entity_list, rel_list in zip(entity_lists_bern2, predicted_rels):\n",
    "        #for report, relations in zip(entity_list, rel_list):\n",
    "            print(entity_list)\n",
    "            print(\"\\n\")\n",
    "            print(rel_list)\n",
    "            G = construct_knowledge_graph(entity_list, rel_list)\n",
    "            G_all = nx.disjoint_union(G_all, G)\n",
    "\n",
    "    visualize_knowledge_graph(G_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_reports_folder():\n",
    "    # Remove existing files in the 'diagnostic_reports' folder\n",
    "    for file in os.listdir('diagnostic_reports'):\n",
    "        os.remove(os.path.join('diagnostic_reports', file))\n",
    "\n",
    "def generate_reports(input_file, num_patients=None):\n",
    "    # Create the 'diagnostic_reports' folder if it doesn't exist\n",
    "    if not os.path.exists('diagnostic_reports'):\n",
    "        os.makedirs('diagnostic_reports')\n",
    "\n",
    "    # Read the CSV file with no truncation\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Limit the number of patients if num_patients is provided\n",
    "    if num_patients:\n",
    "        df = df.head(num_patients)\n",
    "\n",
    "    # Generate .txt reports for each patient\n",
    "    for idx, row in df.iterrows():\n",
    "        # Use the new headers based on your dataset\n",
    "        patient_id = row['subject_id']\n",
    "        \n",
    "        # For each illness_history column, create a separate file if the value is not NaN\n",
    "        for col in df.columns:\n",
    "            if \"illness_history_\" in col and pd.notna(row[col]):\n",
    "                with open(f'diagnostic_reports/#{patient_id}_{col}.txt', 'w', encoding='utf-8') as f:\n",
    "                    f.write(str(row[col]))\n",
    "\n",
    "        preprocessed_reports = preprocess_medical_knowledge()\n",
    "        process_reports(preprocessed_reports)\n",
    "\n",
    "        # Clear the reports folder for the next patient\n",
    "        clear_reports_folder()\n",
    "\n",
    "    print(f\"Reports processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reports_from_dataset(dataset_file):\n",
    "    \"\"\"\n",
    "    Process reports from the specified dataset file.\n",
    "    \"\"\"\n",
    "    generate_reports(input_file=dataset_file, num_patients=None)\n",
    "    preprocessed_reports = preprocess_medical_knowledge()\n",
    "    process_reports(preprocessed_reports)\n",
    "    \n",
    "def process_reports_from_folder():\n",
    "    \"\"\"\n",
    "    Process reports that are manually inserted into the 'diagnostic_reports' folder.\n",
    "    \"\"\"\n",
    "    preprocessed_reports = preprocess_medical_knowledge()\n",
    "    process_reports(preprocessed_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Process medical reports in two modes.')\n",
    "    parser.add_argument('--manual', action='store_true', help='Use manually inserted reports in the diagnostic_reports folder.')\n",
    "    parser.add_argument('--dataset', type=str, help='Specify the path to the dataset file to process reports from.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.manual:\n",
    "        process_reports_from_folder()\n",
    "    elif args.dataset:\n",
    "        process_reports_from_dataset(args.dataset)\n",
    "    else:\n",
    "        print(\"Please specify a mode: --manual for manually inserted reports or --dataset <path_to_dataset> for processing from a dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" candidates = [s for s in parsed_entities if (s.get('entities')) and (len(s['entities']) > 1)]\n",
    "predicted_rels = []\n",
    "print(candidates)\n",
    "print(\"\\n\\n\")\n",
    "for c in candidates:\n",
    "  combinations = itertools.combinations([{'name':x['entity'], 'id':x['entity_id']} for x in c['entities']], 2)\n",
    "  for combination in list(combinations):\n",
    "    try:\n",
    "      ranked_rels = extractor.rank(text=c['text'].replace(\",\", \" \"), head=combination[0]['name'], tail=combination[1]['name'])\n",
    "      if ranked_rels[0][1] > 0.85:\n",
    "        predicted_rels.append({'head': combination[0]['id'], 'tail': combination[1]['id'], 'type':ranked_rels[0][0], 'source': c['text_sha256']})\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "print(predicted_rels) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medicalmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac16b4abddbce4d17bf17701dc5dbeffc561567f91200558127c63e6bb9d7c60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
